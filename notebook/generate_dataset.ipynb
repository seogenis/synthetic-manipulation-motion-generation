{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e2ff454-b14a-4ef1-986c-9087c003ea53",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Imitation learning is a type of machine learning where an agent learns to imitate the actions of an expert. In order to train a robust agent, we need a large amount of data. However, manual data collection through human demonstrations is time-consuming and expensive.\n",
    "\n",
    "Isaac Lab Mimic is a feature included in Isaac Lab that allows users to generate new demonstrations by synthesizing trajectories using a small number of human demonstrations. This blueprint will show how to use Isaac Lab Mimic to generate new motion trajectories for a Franka robotic arm and then visually augment using NVIDIA Cosmos to create datasets for imitation learning. The workflow is broken down into two main steps:\n",
    "\n",
    "1. Generate new demonstrations by synthesizing trajectories using a small number of human demonstrations with Isaac Lab Mimic.\n",
    "2. Apply diverse visual transformations using NVIDIA Cosmos to the new demonstrations to create a large and diverse dataset.\n",
    "\n",
    "This notebook will guide you through each step of the workflow.\n",
    "\n",
    "**NOTE: This notebook must be run on the same machine as the Isaac Sim simulator and a display must be connected to the machine.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5741a94",
   "metadata": {},
   "source": [
    "# Understanding the Blueprint\n",
    "\n",
    "## Motion Trajectory Synthesis\n",
    "Isaac Lab Mimic is a feature set bundled with Isaac Lab (an open source robotic learning framework designed to help train robot policies). The core idea of Mimic is to allow users to synthetically generate a large number of new robot motion trajectories using only a handful of human demonstrations, thus greatly reducing the amount of time and effort required to collect a dataset for imitation learning. \n",
    "\n",
    "Human datasets are annotated with subtask information, which Isaac Lab Mimic uses to construct trajectories for new scene configurations by spatially transforming the original demonstrations.\n",
    "\n",
    "## Visual Augmentation\n",
    "Once generated, the new motion trajectories can be visually augmented using NVIDIA Cosmos to create a diverse dataset that is suitable for training an imitation learning policy. \n",
    "\n",
    "By using multi-staged data generation scheme, we can automatically create a robust dataset for training complex imitation learning policies without the need for large amounts of manual human data, greatly increasing the amount of data available for training and lowering the amount of time required to collect a dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7239a3",
   "metadata": {},
   "source": [
    "# Generate a New Motion Trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f4eee1",
   "metadata": {},
   "source": [
    "\n",
    "## Setup Initial Configuration for Isaac Lab\n",
    "\n",
    "This cell sets up the basic configuration for data generation:\n",
    "\n",
    "1. **How to Modify**:\n",
    "   - Adjust `num_envs` based on your GPU capability\n",
    "   - Set `generation_num_trials` to how many successful trials to execute. Note that some trials may be unsuccessful and so the total number of trials performed may be larger.\n",
    "\n",
    "2. **Tips**:\n",
    "   - Start with 1 trial for testing, increase for training. Increasing trails will increase the time it takes to generate the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d91d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_widgets import create_num_trials_input\n",
    "\n",
    "num_envs = 1\n",
    "num_trials = create_num_trials_input()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc54d6a",
   "metadata": {},
   "source": [
    "## Spin up the Simulation\n",
    "\n",
    "Run this cell to start the simulation environment. This sets up the necessary components for data generation.\n",
    "\n",
    "**NOTE**: When the simulation is running, a **\"Isaac Sim\" is not responding.\"** pop up may appear. This is expected. Please click the **Wait** option and wait while the process completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa2cf0f-cf3c-4383-a233-216ff1f48c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from isaaclab.app import AppLauncher\n",
    "\n",
    "parser = ArgumentParser()\n",
    "AppLauncher.add_app_launcher_args(parser)\n",
    "args_cli = parser.parse_args([])\n",
    "args_cli.enable_cameras = True\n",
    "args_cli.kit_args = \"--enable omni.videoencoding\"\n",
    "\n",
    "config = {\n",
    "    \"task\": \"Isaac-Stack-Cube-Franka-IK-Rel-Blueprint-Mimic-v0\",  \n",
    "    \"num_envs\": num_envs,                                       \n",
    "    \"generation_num_trials\": num_trials.value,                         \n",
    "    \"input_file\": \"datasets/annotated_dataset.hdf5\",     \n",
    "    \"output_file\": \"datasets/generated_dataset.hdf5\", \n",
    "    \"pause_subtask\": False,\n",
    "    \"enable\": \"omni.kit.renderer.capture\",\n",
    "}\n",
    "\n",
    "# Update the default configuration\n",
    "args_dict = vars(args_cli)\n",
    "args_dict.update(config)\n",
    "args_cli = Namespace(**args_dict)\n",
    "\n",
    "# Now launch the simulator with the final configuration\n",
    "app_launcher = AppLauncher(args_cli)\n",
    "simulation_app = app_launcher.app\n",
    "\n",
    "import asyncio\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import isaaclab_mimic.envs  # noqa: F401\n",
    "from isaaclab_mimic.datagen.generation import env_loop, setup_env_config, setup_async_generation\n",
    "from isaaclab_mimic.datagen.utils import get_env_name_from_dataset, setup_output_paths, interactive_update_randomizable_params, reset_env\n",
    "from isaaclab.managers import ObservationTermCfg as ObsTerm\n",
    "from notebook_utils import ISAACLAB_OUTPUT_DIR\n",
    "\n",
    "import isaaclab_tasks  # noqa: F401\n",
    "num_envs = args_cli.num_envs\n",
    "\n",
    "# Setup output paths and get env name\n",
    "output_dir, output_file_name = setup_output_paths(args_cli.output_file)\n",
    "env_name = args_cli.task or get_env_name_from_dataset(args_cli.input_file)\n",
    "\n",
    "# Configure environment\n",
    "env_cfg, success_term = setup_env_config(\n",
    "    env_name=env_name,\n",
    "    output_dir=output_dir,\n",
    "    output_file_name=output_file_name,\n",
    "    num_envs=num_envs,\n",
    "    device=args_cli.device,\n",
    "    generation_num_trials=args_cli.generation_num_trials,\n",
    ")\n",
    "# Set observation output directory\n",
    "for obs in vars(env_cfg.observations.rgb_camera).values():\n",
    "    if not isinstance(obs, ObsTerm):\n",
    "        continue\n",
    "    obs.params[\"image_path\"] = os.path.join(ISAACLAB_OUTPUT_DIR, obs.params[\"image_path\"])\n",
    "env_cfg.observations\n",
    "\n",
    "\n",
    "# create environment\n",
    "env = gym.make(env_name, cfg=env_cfg).unwrapped\n",
    "\n",
    "# set seed for generation\n",
    "random.seed(env.cfg.datagen_config.seed)\n",
    "np.random.seed(env.cfg.datagen_config.seed)\n",
    "torch.manual_seed(env.cfg.datagen_config.seed)\n",
    "\n",
    "# reset before starting\n",
    "reset_env(env, 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd57b1",
   "metadata": {},
   "source": [
    "## Interactive Parameter Updates\n",
    "\n",
    "To get diversity in the generated motion trajectories, the scene configuration is randomized for each trial. This section provides interactive sliders and controls to adjust various environment parameters in real-time:\n",
    "\n",
    "1. **What You'll See**:\n",
    "   - Sliders for numerical values\n",
    "   - Range inputs for min/max settings\n",
    "   - Current value displays\n",
    "   - Parameter names and allowed ranges\n",
    "\n",
    "2. **How to Use**:\n",
    "   - Move the sliders to adjust values\n",
    "   - Watch the environment update in real-time\n",
    "\n",
    "3. **Available Parameters**:\n",
    "   - **Franka Joint State Randomization**:\n",
    "     - **mean (0.0 - 0.5)**: Controls the average joint angle offset (in radians)\n",
    "     - **std (0.0 - 0.1)**: Controls the spread of randomization around the mean\n",
    "\n",
    "   - **Cube Position Randomization**:\n",
    "     - **pose_range.x (0.3 - 0.9)**: Controls cube placement along the x-axis (in meters)\n",
    "     - **pose_range.y (-0.3 - 0.3)**: Controls cube placement along the y-axis (in meters)\n",
    "     - **min_separation (0.0 - 0.5)**: Minimum allowed distance between cubes (in meters)\n",
    "     \n",
    "     **Note:** If the system cannot place cubes with the specified minimum separation after several attempts (due to space constraints), it will accept the last generated positions even if they don't meet the separation requirement. This prevents the system from getting stuck in an impossible configuration.\n",
    "\n",
    "\n",
    "4. **Tips**:\n",
    "   - Start with small adjustments to understand their effects\n",
    "\n",
    "Note: These adjustments will affect how new demonstrations are generated, so take time to experiment with different settings to achieve desired behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27b67f-d051-45e5-878f-2f6e1e6822f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomizable_params = {\n",
    "    \"randomize_franka_joint_state\": {\n",
    "        \"mean\": (0.0, 0.5, 0.01),\n",
    "        \"std\": (0.0, 0.1, 0.01),\n",
    "    },\n",
    "    \"randomize_cube_positions\": {\n",
    "        \"pose_range\": {\n",
    "                \"x\": (0.3, 0.9, 0.01),\n",
    "                \"y\": (-0.3, 0.3, 0.01),\n",
    "            },\n",
    "        \"min_separation\": (0.0, 0.5, 0.01),\n",
    "    }\n",
    "}\n",
    "\n",
    "for i in range(len(env.unwrapped.event_manager._mode_term_cfgs[\"reset\"])):\n",
    "    event_term = env.unwrapped.event_manager._mode_term_cfgs[\"reset\"][i]\n",
    "    name = env.unwrapped.event_manager.active_terms[\"reset\"][i]\n",
    "    display(f\"Updating parameters for event: {event_term.func.__name__}\")\n",
    "    interactive_update_randomizable_params(event_term, name, randomizable_params[name], env=env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d35f42",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "Run this cell to start generating demonstrations using the parameters you've configured. The process will:\n",
    "- Generate the specified number of demonstrations\n",
    "- Save successful demonstrations to your output file\n",
    "- Show progress as demonstrations are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035cd632-b2a5-4e23-8ad4-3df7e32a2512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from IPython.display import display, Pretty\n",
    "\n",
    "# Create a new output capture\n",
    "class OutputCapture:\n",
    "    def __init__(self):\n",
    "        self._buffer = \"\"\n",
    "    \n",
    "    def write(self, text):\n",
    "        if text.strip():  # Only process non-empty strings\n",
    "            display(Pretty(text.rstrip()))\n",
    "    \n",
    "    def flush(self):\n",
    "        if self._buffer:\n",
    "            display(Pretty(self._buffer))\n",
    "            self._buffer = \"\"\n",
    "\n",
    "# Move stdout redirection before setup_async_generation\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = OutputCapture()\n",
    "\n",
    "try:\n",
    "    # Setup and run async data generation\n",
    "    async_components = setup_async_generation(\n",
    "        env=env,\n",
    "        num_envs=args_cli.num_envs,\n",
    "        input_file=args_cli.input_file,\n",
    "        success_term=success_term,\n",
    "        pause_subtask=args_cli.pause_subtask\n",
    "    )\n",
    "\n",
    "    future = asyncio.ensure_future(asyncio.gather(*async_components['tasks']))\n",
    "    env_loop(env, async_components['action_queue'], \n",
    "            async_components['info_pool'], async_components['event_loop'])\n",
    "except asyncio.CancelledError:\n",
    "    display(Pretty(\"Tasks were cancelled.\"))\n",
    "except AttributeError as e:\n",
    "    if \"'FrankaCubeStackIKRelMimicEnv' object has no attribute 'scene'\" in str(e):\n",
    "        display(Pretty(\"Environment was closed during execution. This is expected behavior.\"))\n",
    "except Exception as e:\n",
    "    display(Pretty(f\"Error occurred: {str(e)}\"))\n",
    "finally:\n",
    "    # Restore original stdout first\n",
    "    sys.stdout = old_stdout\n",
    "    \n",
    "    # Cancel the future and ignore any AttributeErrors from pending tasks\n",
    "    if 'future' in locals():\n",
    "        future.cancel()\n",
    "        try:\n",
    "            async_components['event_loop'].run_until_complete(future)\n",
    "        except (asyncio.CancelledError, AttributeError) as e:\n",
    "            if isinstance(e, AttributeError) and \"'FrankaCubeStackIKRelMimicEnv' object has no attribute 'scene'\" in str(e):\n",
    "                display(Pretty(\"Environment was closed during execution. This is expected behavior!\"))\n",
    "            elif isinstance(e, asyncio.CancelledError):\n",
    "                display(Pretty(\"Tasks were properly cancelled during cleanup.\"))\n",
    "            else:\n",
    "                display(Pretty(f\"Unexpected cleanup error: {str(e)}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd54f9-b002-4028-9699-bd85827b98e3",
   "metadata": {},
   "source": [
    "# Cosmos\n",
    "\n",
    "Now that a new motion trajectory has been generated, we will apply visual transformations to the data using Cosmos to create a realistic demo that is suitable for training an imitation learning policy.\n",
    "\n",
    "## Video Preprocessing\n",
    "In this first step, we will process the generated motion trajectory into a video that can be used as an input for the Cosmos model.\n",
    "The normals of the scene are used to apply shading to the semantic segmentation which produces an input that works very well with the Cosmos model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c599698-a590-4eca-a53b-dba6011be42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_widgets import create_camera_input\n",
    "from notebook_utils import ISAACLAB_OUTPUT_DIR\n",
    "\n",
    "VIDEO_LENGTH = 120   # Suggested length is between 120 and 200\n",
    "camera_selection = create_camera_input(ISAACLAB_OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d446c26-d7e3-4515-9818-c8bf4f93da13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Video\n",
    "from notebook_utils import encode_video, ISAACLAB_OUTPUT_DIR, get_env_trial_frames\n",
    "\n",
    "env_trial_frames = get_env_trial_frames(ISAACLAB_OUTPUT_DIR, camera_selection.value, 10)\n",
    "camera = camera_selection.value\n",
    "for env_num, trial_nums in env_trial_frames.items():\n",
    "    for trial_num, (start_frame, end_frame) in trial_nums.items():\n",
    "        trial_length = end_frame - start_frame + 1\n",
    "        if trial_length < VIDEO_LENGTH:\n",
    "            print(f\"\\nSkipping Trial {trial_num}: Too short ({trial_length} frames)\")\n",
    "            continue\n",
    "            \n",
    "        video_start = max(start_frame, end_frame - VIDEO_LENGTH + 1)\n",
    "        \n",
    "        # Generate video filename with trial number\n",
    "        video_filepath = os.path.join(ISAACLAB_OUTPUT_DIR, f\"shaded_segmentation_{camera}_trial_{trial_num}_tile_{env_num}.mp4\")\n",
    "            \n",
    "        try:\n",
    "            encode_video(ISAACLAB_OUTPUT_DIR, video_start, VIDEO_LENGTH, \n",
    "                        camera, video_filepath, env_num, trial_num)\n",
    "            display(video_filepath)\n",
    "            display(Video(video_filepath, width=1000))\n",
    "        except ValueError as e:\n",
    "            print(f\"Error processing trial {trial_num}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa2e8a-0748-4201-b6e2-6582455bea85",
   "metadata": {},
   "source": [
    "## Deploying Cosmos\n",
    "Deploy Cosmos on your provider of choice, or to your own local resources: [Cosmos Transfer1](https://huggingface.co/nvidia/Cosmos-Transfer1-7B). \n",
    "Click on the `Code` link on the Cosmos Transfer page and follow the installation steps outlined in the README. You can find detailed setup instructions in under `examples/inference_multi_control_manual_input.md`.\n",
    "\n",
    "> ### Adding a Web API to Cosmos Transfer1\n",
    "> To simplify testing, copy the file `notebook/app.py` into the Cosmos root directory, and run it with `python app.py`. This will expose endpoints which we'll use to communicate between the notebook and the cosmos model. The script exposes endpoints at port `5000` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdf11a7-4375-478c-a066-f109ce847601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "url_widget = widgets.Text(value=\"\", placeholder=\"cosmos/url:port\", description=\"Cosmos URL:\", style={'description_width': 'initial'}, layout={\"width\": \"1000px\"})\n",
    "display(url_widget)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fe33b4-3943-4716-92cd-e3cbc642ff6d",
   "metadata": {},
   "source": [
    "### Using the Cosmos Model\n",
    "\n",
    "The Cosmos model has several available parameters which alter the output in various ways:\n",
    "- `prompt`: Text prompt for the video generation.\n",
    "- `seed`: Seed for the random number generator. `int [0 - 2147483648]`\n",
    "- `control_weight`: Controls how strongly the control input should affect the output. The stronger the effect, the more adherance to the control input, but the less the model generation freedom. `float [0 - 1.0]`\n",
    "- `sigma_max`: A float value representing the maximum sigma. Lower values result in less change from the original input while a larger values allows for more change but may diverge more from the input scene. `float [0 - 80.0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd47b996-132e-416a-a94d-ba0548bfdcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_widgets import create_variable_dropdowns, create_cosmos_params\n",
    "from notebook_utils import ISAACLAB_OUTPUT_DIR, COSMOS_OUTPUT_DIR\n",
    "\n",
    "prompt_manager = create_variable_dropdowns(\"stacking_prompt.toml\")\n",
    "cosmos_params = create_cosmos_params(ISAACLAB_OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837ad2d0-7fc3-45ad-88ed-748e34b6eafd",
   "metadata": {},
   "source": [
    "## Generate with Cosmos\n",
    "---\n",
    "> **NOTE:** Generation generally takes around 5 to 10 minutes on a single H100 GPU depending on the video length.\n",
    "\n",
    "---\n",
    "\n",
    "> **Tips:**\n",
    "> - To increase prompt adherence, try increasing the `Sigma Max` value\n",
    "> - To reduce divergence from the input scene, try increasing the `Control Weight` and/or increasing `Canny Strength`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff80ae7d-2698-41be-8812-fc1597c208f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cosmos_request import process_video\n",
    "from notebook_utils import ISAACLAB_OUTPUT_DIR\n",
    "from notebook_widgets import create_download_link\n",
    "from IPython.display import Video, clear_output\n",
    "\n",
    "params = {k: w.value for k, w in cosmos_params.items()}\n",
    "video_filepath = os.path.join(ISAACLAB_OUTPUT_DIR, params.pop(\"input_video\"))\n",
    "output_path = f\"{COSMOS_OUTPUT_DIR}/cosmos_{params['seed']}.mp4\"\n",
    "params[\"prompt\"] = prompt_manager.prompt\n",
    "\n",
    "if not url_widget.value:\n",
    "    raise ValueError(\"Enter URL to proceed.\")\n",
    "\n",
    "response = process_video(\n",
    "    url=url_widget.value,\n",
    "    video_path=video_filepath,\n",
    "    output_path=output_path,\n",
    "    **params,\n",
    ")\n",
    "if response is None:\n",
    "    display(\"An error occurred processing the request\")\n",
    "elif response.status_code == 200:\n",
    "    clear_output(wait=True)\n",
    "    display(Video(output_path))\n",
    "    display(create_download_link(output_path, link_text=f\"Download Video: {output_path}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa860c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784bd2fe-e628-4e7d-bcbd-c73bafcf4648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
